{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "text_generation_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatiebalk/text_mining_project/blob/master/text_generation_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF64Ckw53r5i"
      },
      "source": [
        "# Text generation\n",
        "\n",
        "In this notebook text will be generated with multiple different methods, as well as multiple different kinds of text (poetry, news artciles, stories). The generated text will be evaluated using the BERTscore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVEB3Qb93r5q"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os.path import join, isfile\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3tI6V-43r5s",
        "outputId": "7e97a86e-dada-47c4-c8a5-2b48eb63a8cd"
      },
      "source": [
        "import sys\n",
        "print(sys.version)\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EzuTfgvXxHT",
        "outputId": "94cc8d8d-7904-4e33-985c-9fe4c26fdd07"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrtTafY_3r5t"
      },
      "source": [
        "### Harry Potter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxl61KzW3r5u"
      },
      "source": [
        "text = open('/content/gdrive/My Drive/TxMM/harrypotter.txt').read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI6mBXml3r5v"
      },
      "source": [
        "### 1. GRU|\n",
        "From https://medium.com/towards-artificial-intelligence/create-your-own-harry-potter-short-story-using-rnn-and-tensorflow-853b3ed1b8f3 and https://www.tensorflow.org/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGP2GEcc3r5v"
      },
      "source": [
        "# print(text[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMXwI9S13r5w"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\n",
        "index2char = np.array(vocab)\n",
        "text_as_int = np.array([char2index[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRhToSzd3r5w"
      },
      "source": [
        "seq_length = 50\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(data):\n",
        "  input_text = data[:-1]\n",
        "  target_text = data[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v3bb8Y03r5x",
        "outputId": "fb9c3647-4a35-48da-df53-80e654a4e1b6"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "\n",
        "# Number of RNN units \n",
        "rnn_units1 = 512\n",
        "rnn_units2 = 256\n",
        "rnn_units= [rnn_units1, rnn_units2]\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "       batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units1, return_sequences=True,\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.GRU(rnn_units2, return_sequences=True,\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(\n",
        "vocab_size = vocab_size,\n",
        "embedding_dim=embedding_dim,\n",
        "rnn_units=rnn_units,\n",
        "batch_size=BATCH_SIZE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "         logits, from_logits=True)\n",
        "  \n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 300)           31800     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 512)           1250304   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 256)           591360    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 106)           27242     \n",
            "=================================================================\n",
            "Total params: 1,900,706\n",
            "Trainable params: 1,900,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOEclykf3r5x"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/gdrive/My Drive/TxMM/GRU_results/training_checkpoints_GRU_hp'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "   filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty5ChMGN3r5y",
        "outputId": "0b2c3e2a-d4c2-4230-eb81-d81503399c7b"
      },
      "source": [
        "EPOCHS= 50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "latest_check = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1915/1915 [==============================] - 44s 21ms/step - loss: 1.9830 - accuracy: 0.4482\n",
            "Epoch 2/50\n",
            "1915/1915 [==============================] - 41s 21ms/step - loss: 1.3330 - accuracy: 0.6033\n",
            "Epoch 3/50\n",
            "1915/1915 [==============================] - 42s 22ms/step - loss: 1.2774 - accuracy: 0.6175\n",
            "Epoch 4/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2519 - accuracy: 0.6240\n",
            "Epoch 5/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2370 - accuracy: 0.6281\n",
            "Epoch 6/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2255 - accuracy: 0.6308\n",
            "Epoch 7/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2181 - accuracy: 0.6330\n",
            "Epoch 8/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.2131 - accuracy: 0.6342\n",
            "Epoch 9/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.2088 - accuracy: 0.6355\n",
            "Epoch 10/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2053 - accuracy: 0.6363\n",
            "Epoch 11/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2038 - accuracy: 0.6367\n",
            "Epoch 12/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2019 - accuracy: 0.6372\n",
            "Epoch 13/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2011 - accuracy: 0.6372\n",
            "Epoch 14/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.1999 - accuracy: 0.6377\n",
            "Epoch 15/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.1996 - accuracy: 0.6378\n",
            "Epoch 16/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.1995 - accuracy: 0.6376\n",
            "Epoch 17/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.1998 - accuracy: 0.6375\n",
            "Epoch 18/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2005 - accuracy: 0.6373\n",
            "Epoch 19/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2020 - accuracy: 0.6370\n",
            "Epoch 20/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2036 - accuracy: 0.6366\n",
            "Epoch 21/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2053 - accuracy: 0.6358\n",
            "Epoch 22/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2070 - accuracy: 0.6354\n",
            "Epoch 23/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2078 - accuracy: 0.6349\n",
            "Epoch 24/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2099 - accuracy: 0.6345\n",
            "Epoch 25/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2132 - accuracy: 0.6336\n",
            "Epoch 26/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2162 - accuracy: 0.6329\n",
            "Epoch 27/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2197 - accuracy: 0.6317\n",
            "Epoch 28/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2221 - accuracy: 0.6311\n",
            "Epoch 29/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2258 - accuracy: 0.6300\n",
            "Epoch 30/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.2326 - accuracy: 0.6282\n",
            "Epoch 31/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2373 - accuracy: 0.6265\n",
            "Epoch 32/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2449 - accuracy: 0.6244\n",
            "Epoch 33/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2543 - accuracy: 0.6218\n",
            "Epoch 34/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.2677 - accuracy: 0.6180\n",
            "Epoch 35/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.2792 - accuracy: 0.6148\n",
            "Epoch 36/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.2983 - accuracy: 0.6095\n",
            "Epoch 37/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.3519 - accuracy: 0.5945\n",
            "Epoch 38/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.3978 - accuracy: 0.5815\n",
            "Epoch 39/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.5073 - accuracy: 0.5514\n",
            "Epoch 40/50\n",
            "1915/1915 [==============================] - 44s 22ms/step - loss: 1.5605 - accuracy: 0.5372\n",
            "Epoch 41/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5518 - accuracy: 0.5404\n",
            "Epoch 42/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5801 - accuracy: 0.5327\n",
            "Epoch 43/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5380 - accuracy: 0.5444\n",
            "Epoch 44/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5105 - accuracy: 0.5521\n",
            "Epoch 45/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.4879 - accuracy: 0.5583\n",
            "Epoch 46/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5330 - accuracy: 0.5456\n",
            "Epoch 47/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5931 - accuracy: 0.5292\n",
            "Epoch 48/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.6051 - accuracy: 0.5264\n",
            "Epoch 49/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5670 - accuracy: 0.5369\n",
            "Epoch 50/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5305 - accuracy: 0.5472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_MRslGT3r5y",
        "outputId": "46d48bc8-9428-430d-c386-bae1c5c6e9b4"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(latest_check)\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 300)            31800     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 512)            1250304   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 256)            591360    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 106)            27242     \n",
            "=================================================================\n",
            "Total params: 1,900,706\n",
            "Trainable params: 1,900,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqfLki_Y3r5z"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 1000  #can be anything you like\n",
        "  input_eval = [char2index[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  scaling = 0.5 #kept at a lower value here\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / scaling\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # Pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(index2char[predicted_id])\n",
        "\n",
        "  return (start_string + \"\".join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nylPCkV_3r5z",
        "outputId": "23902120-65af-40b8-9465-6b9e7add2bb8"
      },
      "source": [
        "print(generate_text(model, start_string= \"Severus Snape\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Severus Snape was about the bank. And they spectain to the wand and her from them behind him again on the side shock to a good or his face formite that the should face the first of the Mudble soundle and the end the gravents was sister from the parch of the scape be close that he could not thought the sounding and the strang and the dark for them and she had the past that the sward of Ron was start and a long study and surprised and shaker of the sittly fall and the travels and more was thing of the stood sharish, and they are the door on the suspected him.\n",
            "\"Who into the close to see the thing that an experious of Magic that the said and fell when the parting and Grey could not the she was not seem to Griphoop. And he was in the pocter on the same back over his face of the thought he moved to stude the stop in the stair of the potion. He was a destronumition of the stood the blumbled and a laugher finer and some of she saw the secide somether to call of the spart of the bitter of his face and Harry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85n1CriWYK4b",
        "outputId": "c75c1115-6c3e-430a-9263-355a876c7d8f"
      },
      "source": [
        "# open prompts file\r\n",
        "prompts = open('/content/gdrive/My Drive/TxMM/hp_prompts.txt', 'r').readlines()\r\n",
        "generated_dict = {}\r\n",
        "\r\n",
        "for i, p in tqdm(enumerate(prompts)):\r\n",
        "  # generate 1000 characters\r\n",
        "  gen_text = generate_text(model, start_string=p)\r\n",
        "  generated_dict[i] = gen_text\r\n",
        "\r\n",
        "with open('/content/gdrive/My Drive/TxMM/GRU_results/hp_generated_texts.json', 'w') as fp:\r\n",
        "  json.dump(generated_dict, fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100it [10:57,  6.58s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEo2SKYxxaaw"
      },
      "source": [
        "### News data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAKruDpW3r51",
        "outputId": "e5a35a56-340f-4336-ca7b-9c82c1620f06"
      },
      "source": [
        "news_dir = '/content/gdrive/My Drive/TxMM/news_data'\r\n",
        "files = [f for f in listdir(news_dir) if isfile(join(news_dir, f))]\r\n",
        "text = \"\"\r\n",
        "\r\n",
        "csv.field_size_limit(sys.maxsize)\r\n",
        "\r\n",
        "for f in files:\r\n",
        "  with open(join(news_dir, f)) as csv_file:\r\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\r\n",
        "    line_count = 0\r\n",
        "    for row in tqdm(csv_reader):\r\n",
        "      if line_count == 0:\r\n",
        "        print(f'Column names are {\", \".join(row)}')\r\n",
        "        line_count += 1\r\n",
        "      else:\r\n",
        "        text = text + \" \" + row[-1] + \" \"\r\n",
        "        line_count += 1\r\n",
        "  break\r\n",
        "\r\n",
        "with open('/content/gdrive/My Drive/TxMM/news_data.txt', 'w') as outfile:\r\n",
        "    \r\n",
        "    outfile.write(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132it [00:00,  5.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Column names are , id, title, publication, author, date, year, month, url, content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "50001it [3:20:35,  4.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PezSl1GD0xWn"
      },
      "source": [
        "vocab = sorted(set(text))\r\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\r\n",
        "index2char = np.array(vocab)\r\n",
        "text_as_int = np.array([char2index[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjKc2BjV2RYc"
      },
      "source": [
        "seq_length = 50\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\r\n",
        "\r\n",
        "def split_input_target(data):\r\n",
        "  input_text = data[:-1]\r\n",
        "  target_text = data[1:]\r\n",
        "  return input_text, target_text\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)\r\n",
        "\r\n",
        "BATCH_SIZE = 64\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PrRIlZw2TuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb62e4ca-bed3-4002-b022-7d5cb930b8e2"
      },
      "source": [
        "vocab_size = len(vocab)\r\n",
        "embedding_dim = 300\r\n",
        "\r\n",
        "# Number of RNN units \r\n",
        "rnn_units1 = 512\r\n",
        "rnn_units2 = 256\r\n",
        "rnn_units= [rnn_units1, rnn_units2]\r\n",
        "\r\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "  model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n",
        "       batch_input_shape=[batch_size, None]),\r\n",
        "    tf.keras.layers.GRU(rnn_units1, return_sequences=True,\r\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\r\n",
        "    tf.keras.layers.GRU(rnn_units2, return_sequences=True,\r\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\r\n",
        "    tf.keras.layers.Dense(vocab_size)\r\n",
        "  ])\r\n",
        "  return model\r\n",
        "\r\n",
        "model = build_model(\r\n",
        "vocab_size = vocab_size,\r\n",
        "embedding_dim=embedding_dim,\r\n",
        "rnn_units=rnn_units,\r\n",
        "batch_size=BATCH_SIZE)\r\n",
        "\r\n",
        "def loss(labels, logits):\r\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\r\n",
        "         logits, from_logits=True)\r\n",
        "  \r\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 300)           300600    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 512)           1250304   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 256)           591360    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 1002)          257514    \n",
            "=================================================================\n",
            "Total params: 2,399,778\n",
            "Trainable params: 2,399,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwmwkfBu2T0F"
      },
      "source": [
        "# Directory where the checkpoints will be saved\r\n",
        "checkpoint_dir = '/content/gdrive/My Drive/TxMM/GRU_results/training_checkpoints_GRU_news'\r\n",
        "\r\n",
        "# Name of the checkpoint files\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\r\n",
        "   filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiyP7T_j2T2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaeae83a-6bb2-4f56-bfff-304bd8dea4b1"
      },
      "source": [
        "EPOCHS= 50\r\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\r\n",
        "latest_check = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "59060/59060 [==============================] - 1600s 27ms/step - loss: 1.7411 - accuracy: 0.5072\n",
            "Epoch 2/50\n",
            "59060/59060 [==============================] - 1597s 27ms/step - loss: 1.4119 - accuracy: 0.5890\n",
            "Epoch 3/50\n",
            "59060/59060 [==============================] - 1596s 27ms/step - loss: 1.3943 - accuracy: 0.5932\n",
            "Epoch 4/50\n",
            "59060/59060 [==============================] - 1599s 27ms/step - loss: 1.3932 - accuracy: 0.5932\n",
            "Epoch 5/50\n",
            "59060/59060 [==============================] - 1595s 27ms/step - loss: 1.4035 - accuracy: 0.5900\n",
            "Epoch 6/50\n",
            "59060/59060 [==============================] - 1596s 27ms/step - loss: 1.6018 - accuracy: 0.5336\n",
            "Epoch 7/50\n",
            "59060/59060 [==============================] - 1631s 28ms/step - loss: 1.6774 - accuracy: 0.5140\n",
            "Epoch 8/50\n",
            "59060/59060 [==============================] - 1618s 27ms/step - loss: 1.5197 - accuracy: 0.5584\n",
            "Epoch 9/50\n",
            "59060/59060 [==============================] - 1623s 27ms/step - loss: 1.4943 - accuracy: 0.5653\n",
            "Epoch 10/50\n",
            "59060/59060 [==============================] - 1617s 27ms/step - loss: 1.4835 - accuracy: 0.5683\n",
            "Epoch 11/50\n",
            "59060/59060 [==============================] - 1626s 28ms/step - loss: 1.4772 - accuracy: 0.5699\n",
            "Epoch 12/50\n",
            "59060/59060 [==============================] - 1617s 27ms/step - loss: 1.4741 - accuracy: 0.5707\n",
            "Epoch 13/50\n",
            "59060/59060 [==============================] - 1612s 27ms/step - loss: 1.4713 - accuracy: 0.5715\n",
            "Epoch 14/50\n",
            "59060/59060 [==============================] - 1616s 27ms/step - loss: 1.4705 - accuracy: 0.5717\n",
            "Epoch 15/50\n",
            "57665/59060 [============================>.] - ETA: 38s - loss: 1.4697 - accuracy: 0.5718"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLtWjNX2aox"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n",
        "model.load_weights(latest_check)\r\n",
        "model.build(tf.TensorShape([1, None]))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo6E6Un62auV"
      },
      "source": [
        "# open prompts file\r\n",
        "prompts = open('/content/gdrive/My Drive/TxMM/news_prompts.txt', 'r').readlines()\r\n",
        "generated_dict = {}\r\n",
        "\r\n",
        "for i, p in tqdm(enumerate(prompts)):\r\n",
        "  # generate 1000 characters\r\n",
        "  gen_text = generate_text(model, start_string=p)\r\n",
        "  generated_dict[i] = gen_text\r\n",
        "\r\n",
        "with open('/content/gdrive/My Drive/TxMM/GRU_results/hp_generated_texts.json', 'w') as fp:\r\n",
        "  json.dump(generated_dict, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MJdJoKt2awb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}