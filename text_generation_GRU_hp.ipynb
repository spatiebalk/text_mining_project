{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "text_generation_GRU_hp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatiebalk/text_mining_project/blob/master/text_generation_GRU_hp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF64Ckw53r5i"
      },
      "source": [
        "# Text generation GRU with Harry Potter text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVEB3Qb93r5q"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os.path import join, isfile\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import sys"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3tI6V-43r5s",
        "outputId": "fe584575-4e46-4a29-ff50-ef8ef1525fd1"
      },
      "source": [
        "import sys\n",
        "print(sys.version)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EzuTfgvXxHT",
        "outputId": "b8e2d396-3aa0-40af-ca3e-9fb2b762272b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrtTafY_3r5t"
      },
      "source": [
        "### Harry Potter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxl61KzW3r5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb22678-242f-49e5-f231-8ea9f98732a6"
      },
      "source": [
        "text = open('/content/gdrive/My Drive/TxMM/harrypotter.txt').read()\r\n",
        "print(len(text))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6251649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI6mBXml3r5v"
      },
      "source": [
        "### 1. GRU\n",
        "From https://medium.com/towards-artificial-intelligence/create-your-own-harry-potter-short-story-using-rnn-and-tensorflow-853b3ed1b8f3 and https://www.tensorflow.org/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGP2GEcc3r5v"
      },
      "source": [
        "# print(text[:200])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMXwI9S13r5w"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\n",
        "index2char = np.array(vocab)\n",
        "text_as_int = np.array([char2index[c] for c in text])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRhToSzd3r5w"
      },
      "source": [
        "seq_length = 50\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(data):\n",
        "  input_text = data[:-1]\n",
        "  target_text = data[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v3bb8Y03r5x",
        "outputId": "fdf4e077-08c2-4eb9-d697-73d101201924"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "\n",
        "# Number of RNN units \n",
        "rnn_units1 = 512\n",
        "rnn_units2 = 256\n",
        "rnn_units= [rnn_units1, rnn_units2]\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "       batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units1, return_sequences=True,\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.GRU(rnn_units2, return_sequences=True,\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(\n",
        "vocab_size = vocab_size,\n",
        "embedding_dim=embedding_dim,\n",
        "rnn_units=rnn_units,\n",
        "batch_size=BATCH_SIZE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "         logits, from_logits=True)\n",
        "  \n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 300)           31800     \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (64, None, 512)           1250304   \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (64, None, 256)           591360    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 106)           27242     \n",
            "=================================================================\n",
            "Total params: 1,900,706\n",
            "Trainable params: 1,900,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOEclykf3r5x"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/gdrive/My Drive/TxMM/GRU_results/training_checkpoints_GRU_hp'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "   filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty5ChMGN3r5y",
        "outputId": "2de4d830-b51f-4798-e167-b08567d41071"
      },
      "source": [
        "EPOCHS= 50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "latest_check = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1915/1915 [==============================] - 46s 23ms/step - loss: 1.9441 - accuracy: 0.4579\n",
            "Epoch 2/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.3308 - accuracy: 0.6034\n",
            "Epoch 3/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2778 - accuracy: 0.6172\n",
            "Epoch 4/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2527 - accuracy: 0.6237\n",
            "Epoch 5/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2373 - accuracy: 0.6279\n",
            "Epoch 6/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2278 - accuracy: 0.6305\n",
            "Epoch 7/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2203 - accuracy: 0.6323\n",
            "Epoch 8/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2153 - accuracy: 0.6340\n",
            "Epoch 9/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2111 - accuracy: 0.6349\n",
            "Epoch 10/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2091 - accuracy: 0.6351\n",
            "Epoch 11/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.2061 - accuracy: 0.6360\n",
            "Epoch 12/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2045 - accuracy: 0.6362\n",
            "Epoch 13/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2038 - accuracy: 0.6367\n",
            "Epoch 14/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2036 - accuracy: 0.6371\n",
            "Epoch 15/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2028 - accuracy: 0.6367\n",
            "Epoch 16/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2029 - accuracy: 0.6368\n",
            "Epoch 17/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2042 - accuracy: 0.6365\n",
            "Epoch 18/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2044 - accuracy: 0.6363\n",
            "Epoch 19/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2057 - accuracy: 0.6360\n",
            "Epoch 20/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2071 - accuracy: 0.6354\n",
            "Epoch 21/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2084 - accuracy: 0.6352\n",
            "Epoch 22/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2111 - accuracy: 0.6344\n",
            "Epoch 23/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2121 - accuracy: 0.6341\n",
            "Epoch 24/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2145 - accuracy: 0.6334\n",
            "Epoch 25/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2172 - accuracy: 0.6324\n",
            "Epoch 26/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2214 - accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2246 - accuracy: 0.6303\n",
            "Epoch 28/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2291 - accuracy: 0.6290\n",
            "Epoch 29/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2327 - accuracy: 0.6279\n",
            "Epoch 30/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2392 - accuracy: 0.6262\n",
            "Epoch 31/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2468 - accuracy: 0.6242\n",
            "Epoch 32/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2559 - accuracy: 0.6213\n",
            "Epoch 33/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2709 - accuracy: 0.6172\n",
            "Epoch 34/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.2822 - accuracy: 0.6141\n",
            "Epoch 35/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.3118 - accuracy: 0.6058\n",
            "Epoch 36/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.3866 - accuracy: 0.5848\n",
            "Epoch 37/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.5003 - accuracy: 0.5528\n",
            "Epoch 38/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5641 - accuracy: 0.5366\n",
            "Epoch 39/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.5170 - accuracy: 0.5501\n",
            "Epoch 40/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.5554 - accuracy: 0.5397\n",
            "Epoch 41/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5227 - accuracy: 0.5488\n",
            "Epoch 42/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5357 - accuracy: 0.5451\n",
            "Epoch 43/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.5273 - accuracy: 0.5475\n",
            "Epoch 44/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.5067 - accuracy: 0.5537\n",
            "Epoch 45/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.5138 - accuracy: 0.5511\n",
            "Epoch 46/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.4925 - accuracy: 0.5574\n",
            "Epoch 47/50\n",
            "1915/1915 [==============================] - 45s 23ms/step - loss: 1.4746 - accuracy: 0.5620\n",
            "Epoch 48/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.4647 - accuracy: 0.5646\n",
            "Epoch 49/50\n",
            "1915/1915 [==============================] - 44s 23ms/step - loss: 1.4766 - accuracy: 0.5616\n",
            "Epoch 50/50\n",
            "1915/1915 [==============================] - 43s 22ms/step - loss: 1.4585 - accuracy: 0.5667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_MRslGT3r5y",
        "outputId": "148b7cf8-8df5-4296-fc14-57b9c6a46ea4"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(latest_check)\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 300)            31800     \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (1, None, 512)            1250304   \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (1, None, 256)            591360    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 106)            27242     \n",
            "=================================================================\n",
            "Total params: 1,900,706\n",
            "Trainable params: 1,900,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqfLki_Y3r5z"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 1000  #can be anything you like\n",
        "  input_eval = [char2index[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  scaling = 0.5 #kept at a lower value here\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / scaling\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # Pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(index2char[predicted_id])\n",
        "\n",
        "  return (start_string + \"\".join(text_generated))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nylPCkV_3r5z",
        "outputId": "e33c0bc6-9262-452d-cfc2-19f4aa5caa86"
      },
      "source": [
        "print(generate_text(model, start_string= \"Severus Snape\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Severus Snape to be an an indeather to his wand was all, the wand shouting the golden tiny of the office of the counter and had said behind the wand. He could think of the softly who closed to come has he saw that a corner and looked at the darkness upon his head, and he could have told her wand.\n",
            "\"I don’t think what it was a second of the back to the last that it was entered it, and he had the creak a little of the room, and a creath and tried to be some of the air of the staircase, and Harry was a silent to the and the wand had been stocked the whole the car so the part of the window to the door be the sword in the castle on the sign in the silver closed his wand to the concealed the house and felt he had been at his finger of the trait. \"I don’t realized his last of the staircase all the tiny careft that I can one himself of the second in a moment that he was the very wand that he had took a long little side of there the possible and contruffled the contactly place to be containly realize a passe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85n1CriWYK4b",
        "outputId": "7e05e2df-95d3-4de6-9ac9-19686c46cbc0"
      },
      "source": [
        "# open prompts file\r\n",
        "prompts = open('/content/gdrive/My Drive/TxMM/hp_prompts.txt', 'r').readlines()\r\n",
        "generated_dict = {}\r\n",
        "\r\n",
        "for i, p in tqdm(enumerate(prompts)):\r\n",
        "  # generate 1000 characters\r\n",
        "  gen_text = generate_text(model, start_string=p)\r\n",
        "  generated_dict[i] = gen_text\r\n",
        "\r\n",
        "with open('/content/gdrive/My Drive/TxMM/GRU_results/hp_generated_texts.json', 'w') as fp:\r\n",
        "  json.dump(generated_dict, fp)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:06,  6.30s/it]\u001b[A\n",
            "2it [00:12,  6.28s/it]\u001b[A\n",
            "3it [00:18,  6.29s/it]\u001b[A\n",
            "4it [00:25,  6.31s/it]\u001b[A\n",
            "5it [00:31,  6.29s/it]\u001b[A\n",
            "6it [00:37,  6.27s/it]\u001b[A\n",
            "7it [00:43,  6.24s/it]\u001b[A\n",
            "8it [00:49,  6.22s/it]\u001b[A\n",
            "9it [00:56,  6.20s/it]\u001b[A\n",
            "10it [01:02,  6.19s/it]\u001b[A\n",
            "11it [01:08,  6.17s/it]\u001b[A\n",
            "12it [01:14,  6.15s/it]\u001b[A\n",
            "13it [01:20,  6.14s/it]\u001b[A\n",
            "14it [01:26,  6.13s/it]\u001b[A\n",
            "15it [01:33,  6.29s/it]\u001b[A\n",
            "16it [01:39,  6.25s/it]\u001b[A\n",
            "17it [01:45,  6.23s/it]\u001b[A\n",
            "18it [01:51,  6.20s/it]\u001b[A\n",
            "19it [01:58,  6.22s/it]\u001b[A\n",
            "20it [02:04,  6.32s/it]\u001b[A\n",
            "21it [02:11,  6.35s/it]\u001b[A\n",
            "22it [02:17,  6.30s/it]\u001b[A\n",
            "23it [02:23,  6.24s/it]\u001b[A\n",
            "24it [02:29,  6.19s/it]\u001b[A\n",
            "25it [02:35,  6.17s/it]\u001b[A\n",
            "26it [02:41,  6.15s/it]\u001b[A\n",
            "27it [02:47,  6.13s/it]\u001b[A\n",
            "28it [02:53,  6.12s/it]\u001b[A\n",
            "29it [03:00,  6.13s/it]\u001b[A\n",
            "30it [03:06,  6.13s/it]\u001b[A\n",
            "31it [03:12,  6.10s/it]\u001b[A\n",
            "32it [03:18,  6.10s/it]\u001b[A\n",
            "33it [03:24,  6.09s/it]\u001b[A\n",
            "34it [03:30,  6.08s/it]\u001b[A\n",
            "35it [03:36,  6.08s/it]\u001b[A\n",
            "36it [03:42,  6.09s/it]\u001b[A\n",
            "37it [03:48,  6.08s/it]\u001b[A\n",
            "38it [03:54,  6.06s/it]\u001b[A\n",
            "39it [04:00,  6.06s/it]\u001b[A\n",
            "40it [04:06,  6.05s/it]\u001b[A\n",
            "41it [04:12,  6.06s/it]\u001b[A\n",
            "42it [04:18,  6.08s/it]\u001b[A\n",
            "43it [04:25,  6.10s/it]\u001b[A\n",
            "44it [04:31,  6.09s/it]\u001b[A\n",
            "45it [04:37,  6.07s/it]\u001b[A\n",
            "46it [04:43,  6.08s/it]\u001b[A\n",
            "47it [04:49,  6.11s/it]\u001b[A\n",
            "48it [04:55,  6.17s/it]\u001b[A\n",
            "49it [05:02,  6.19s/it]\u001b[A\n",
            "50it [05:08,  6.23s/it]\u001b[A\n",
            "51it [05:14,  6.26s/it]\u001b[A\n",
            "52it [05:20,  6.27s/it]\u001b[A\n",
            "53it [05:27,  6.30s/it]\u001b[A\n",
            "54it [05:33,  6.31s/it]\u001b[A\n",
            "55it [05:40,  6.35s/it]\u001b[A\n",
            "56it [05:46,  6.39s/it]\u001b[A\n",
            "57it [05:52,  6.38s/it]\u001b[A\n",
            "58it [05:59,  6.39s/it]\u001b[A\n",
            "59it [06:05,  6.41s/it]\u001b[A\n",
            "60it [06:12,  6.44s/it]\u001b[A\n",
            "61it [06:18,  6.45s/it]\u001b[A\n",
            "62it [06:25,  6.46s/it]\u001b[A\n",
            "63it [06:31,  6.45s/it]\u001b[A\n",
            "64it [06:38,  6.44s/it]\u001b[A\n",
            "65it [06:44,  6.43s/it]\u001b[A\n",
            "66it [06:50,  6.41s/it]\u001b[A\n",
            "67it [06:57,  6.40s/it]\u001b[A\n",
            "68it [07:03,  6.42s/it]\u001b[A\n",
            "69it [07:10,  6.45s/it]\u001b[A\n",
            "70it [07:17,  6.55s/it]\u001b[A\n",
            "71it [07:23,  6.48s/it]\u001b[A\n",
            "72it [07:29,  6.40s/it]\u001b[A\n",
            "73it [07:35,  6.33s/it]\u001b[A\n",
            "74it [07:41,  6.29s/it]\u001b[A\n",
            "75it [07:48,  6.25s/it]\u001b[A\n",
            "76it [07:54,  6.23s/it]\u001b[A\n",
            "77it [08:00,  6.22s/it]\u001b[A\n",
            "78it [08:06,  6.19s/it]\u001b[A\n",
            "79it [08:12,  6.18s/it]\u001b[A\n",
            "80it [08:19,  6.20s/it]\u001b[A\n",
            "81it [08:25,  6.20s/it]\u001b[A\n",
            "82it [08:31,  6.17s/it]\u001b[A\n",
            "83it [08:37,  6.15s/it]\u001b[A\n",
            "84it [08:43,  6.13s/it]\u001b[A\n",
            "85it [08:49,  6.13s/it]\u001b[A\n",
            "86it [08:55,  6.13s/it]\u001b[A\n",
            "87it [09:01,  6.12s/it]\u001b[A\n",
            "88it [09:07,  6.11s/it]\u001b[A\n",
            "89it [09:14,  6.10s/it]\u001b[A\n",
            "90it [09:20,  6.09s/it]\u001b[A\n",
            "91it [09:26,  6.09s/it]\u001b[A\n",
            "92it [09:32,  6.09s/it]\u001b[A\n",
            "93it [09:38,  6.10s/it]\u001b[A\n",
            "94it [09:44,  6.12s/it]\u001b[A\n",
            "95it [09:50,  6.10s/it]\u001b[A\n",
            "96it [09:56,  6.10s/it]\u001b[A\n",
            "97it [10:02,  6.08s/it]\u001b[A\n",
            "98it [10:08,  6.06s/it]\u001b[A\n",
            "99it [10:14,  6.06s/it]\u001b[A\n",
            "100it [10:20,  6.21s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MJdJoKt2awb"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}